{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder with MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_encoder(x, zdim):\n",
    "    e = tf.layers.dense(x, 250, activation=tf.nn.relu)\n",
    "    mu = tf.layers.dense(e, zdim)\n",
    "    log_sigma = tf.layers.dense(e, zdim)\n",
    "    return mu, log_sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling_op(distribution):\n",
    "    mu, log_sigma = distribution\n",
    "    epsilon = tf.random_normal(shape=tf.shape(mu))\n",
    "    z = mu + tf.exp(log_sigma) * epsilon\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_decoder(z):\n",
    "    d = tf.layers.dense(z, 250, activation=tf.nn.relu)\n",
    "    out = tf.layers.dense(d, 784, activation=tf.sigmoid)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving image samples\n",
    "def save_sample(images_array, filename, shape):\n",
    "    \n",
    "    img_width = images_array.shape[1]\n",
    "    img_height = images_array.shape[2]\n",
    "    \n",
    "    final_width = img_width * shape[0]\n",
    "    final_height = img_width * shape[1]\n",
    "    \n",
    "    final_arr = np.zeros((final_width, final_height))\n",
    "    \n",
    "    for i in range(len(images_array)):\n",
    "        x = int(i % shape[0]) * img_width\n",
    "        y = int(i / shape[0]) * img_height\n",
    "        \n",
    "        final_arr[x:x + img_width, y:y + img_height] = images_array[i].reshape(img_height, img_width)\n",
    "        \n",
    "    final_img = Image.fromarray((final_arr * 255).astype(np.uint8), mode=\"L\")\n",
    "    final_img.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST-data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Model\n",
    "tf.reset_default_graph()\n",
    "\n",
    "zdim = 2\n",
    "beta = 1\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "X = tf.placeholder(dtype=tf.float32, shape=[None, 784])\n",
    "\n",
    "encoder = create_encoder(X, zdim)\n",
    "sampling = sampling_op(encoder)\n",
    "Xh = create_decoder(sampling)\n",
    "\n",
    "# Binary Crossentropy\n",
    "r_loss = -tf.reduce_sum(X * tf.log(1e-8 + Xh) + (1 - X) * tf.log(1e-8 + 1 - Xh), 1)\n",
    "\n",
    "mu, log_sigma = encoder\n",
    "sigma = tf.exp(log_sigma)\n",
    "kl_div = beta * -tf.reduce_sum(tf.square(mu) + tf.square(sigma) - tf.log(1e-8 + tf.square(sigma)), 1)\n",
    "\n",
    "loss = tf.reduce_mean(r_loss) - tf.reduce_mean(kl_div)\n",
    "train = tf.train.RMSPropOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "epochs = 10000\n",
    "display_step = 10\n",
    "\n",
    "test_xs, _ = mnist.test.next_batch(25)\n",
    "save_sample(test_xs.reshape(-1, 28, 28), \"Generated/target.bmp\", [5, 5])\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    xs, _ = mnist.train.next_batch(batch_size)\n",
    "    \n",
    "    sess.run(train, feed_dict={X: xs})\n",
    "    \n",
    "    if epoch % display_step == 0:\n",
    "        print(\"Epoch\", epoch, \"Loss\", sess.run(loss, feed_dict={X: xs}))\n",
    "        a = sess.run(Xh, feed_dict={X: test_xs}).reshape(-1, 28, 28)\n",
    "        save_sample(a, \"Generated/\" + str(epoch) + \".bmp\", [5, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show latent z vector for different MNIST digit classes\n",
    "plot_xs, plot_ys = mnist.test.next_batch(3000)\n",
    "\n",
    "\n",
    "sampled_zvector = sess.run(sampling, feed_dict={X: plot_xs})\n",
    "    \n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.scatter(sampled_zvector[:, 0], sampled_zvector[:, 1], c=plot_ys)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
